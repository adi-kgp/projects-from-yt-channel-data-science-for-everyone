{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dfae231-acdf-4926-8c86-3d0d3a1d3884",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing with PySpark\n",
    "\n",
    "### NLP Tools\n",
    "\n",
    "1. Tokenizer\n",
    "2. Stop word Removal\n",
    "3. n-grams\n",
    "4. Term frequency-inverse document frequency (TF-IDF)\n",
    "5. Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6e7d3-6a2a-4bdc-b402-b95739ce8ba7",
   "metadata": {},
   "source": [
    "### SMS Spam Collection Dataset\n",
    "\n",
    "* Download the data for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f92d3d89-fc8f-47b3-94fd-1d64672afe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  198k    0  198k    0     0  72371      0 --:--:--  0:00:02 --:--:-- 72363\n"
     ]
    }
   ],
   "source": [
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip >> smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "536c810a-0238-4fc5-a582-5777c3aee8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  smsspamcollection.zip\n",
      "  inflating: SMSSpamCollection       \n",
      "  inflating: readme                  \n"
     ]
    }
   ],
   "source": [
    "!unzip smsspamcollection.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35406f57-138c-417c-8b26-f40d8963af0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45966155-12d3-4782-8cdf-7e262d3f0797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/22 23:22:34 WARN Utils: Your hostname, aditya-HP resolves to a loopback address: 127.0.1.1; using 10.103.4.167 instead (on interface wlo1)\n",
      "24/09/22 23:22:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/22 23:22:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"nlp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21b84b4-ad1e-4941-a4b3-3176cf6938ad",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28f956b9-57f8-4eef-975b-a707fec31991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf \n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a21182e-ef4b-4c65-8b14-38bb1ac9557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = spark.createDataFrame(\n",
    "    [\n",
    "        (0, \"Hello I am happy to be learning Apache Spark\"),\n",
    "        (1, \"I enjoy learning about Python and javascript Programming\"),\n",
    "        (2, \"I am familiar with machine learning applications\"),\n",
    "        (3, \"here,is,a,list,of,words\")\n",
    "    ],\n",
    "    [ 'id', 'sentence']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37434936-1ffa-4d3e-821d-5005b5f5192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hello I am happy ...|\n",
      "|  1|I enjoy learning ...|\n",
      "|  2|I am familiar wit...|\n",
      "|  3|here,is,a,list,of...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41dbcfb3-6f51-4805-80e9-07f1eb73588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\") # regex for whole word\n",
    "\n",
    "countTokens = udf(lambda w: len(w), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5aa0032-e71e-42cd-9a20-5366df516e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.transform(sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1171e38c-e310-4445-84d7-725e4ade5079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hello I am happy ...|[hello, i, am, ha...|\n",
      "|  1|I enjoy learning ...|[i, enjoy, learni...|\n",
      "|  2|I am familiar wit...|[i, am, familiar,...|\n",
      "|  3|here,is,a,list,of...|[here,is,a,list,o...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6b62dfb-81fc-4655-8ee5-4d5935b02d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|            sentence|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|Hello I am happy ...|[hello, i, am, ha...|     9|\n",
      "|I enjoy learning ...|[i, enjoy, learni...|     8|\n",
      "|I am familiar wit...|[i, am, familiar,...|     7|\n",
      "|here,is,a,list,of...|[here,is,a,list,o...|     1|\n",
      "+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93bd906c-1007-49f0-98fb-f011ec4fcb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|            sentence|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|Hello I am happy ...|[hello, i, am, ha...|     9|\n",
      "|I enjoy learning ...|[i, enjoy, learni...|     8|\n",
      "|I am familiar wit...|[i, am, familiar,...|     7|\n",
      "|here,is,a,list,of...|[here, is, a, lis...|     6|\n",
      "+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(sent_df)\n",
    "regexTokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b63a60fa-c322-44b8-9daf-f47757e0d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df_token = regexTokenized.select(\"sentence\", \"words\").withColumn(\"tokens\", countTokens(col(\"words\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0769c0-aa6c-4069-9f73-c462b66be0d9",
   "metadata": {},
   "source": [
    "### Stop Words Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b76b732d-6848-4423-8592-be4f961caa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f93090c-077f-40a9-89dc-0c910373730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------------------------+\n",
      "|id |sentence                                                |\n",
      "+---+--------------------------------------------------------+\n",
      "|0  |Hello I am happy to be learning Apache Spark            |\n",
      "|1  |I enjoy learning about Python and javascript Programming|\n",
      "|2  |I am familiar with machine learning applications        |\n",
      "|3  |here,is,a,list,of,words                                 |\n",
      "+---+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23d4aa22-d020-45cf-b886-867444ee17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7491bb39-bb5e-4d67-97f3-3bcaf03e213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+-----------------------------------------------------------------+------+--------------------------------------------------+\n",
      "|sentence                                                |words                                                            |tokens|cleaned                                           |\n",
      "+--------------------------------------------------------+-----------------------------------------------------------------+------+--------------------------------------------------+\n",
      "|Hello I am happy to be learning Apache Spark            |[hello, i, am, happy, to, be, learning, apache, spark]           |9     |[hello, happy, learning, apache, spark]           |\n",
      "|I enjoy learning about Python and javascript Programming|[i, enjoy, learning, about, python, and, javascript, programming]|8     |[enjoy, learning, python, javascript, programming]|\n",
      "|I am familiar with machine learning applications        |[i, am, familiar, with, machine, learning, applications]         |7     |[familiar, machine, learning, applications]       |\n",
      "|here,is,a,list,of,words                                 |[here, is, a, list, of, words]                                   |6     |[list, words]                                     |\n",
      "+--------------------------------------------------------+-----------------------------------------------------------------+------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover.transform(sent_df_token).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb762d-6f51-4fc3-bf95-1c6f42210677",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5389c661-6f4b-4bda-9350-40dbf531f4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "41f54b56-d271-4747-9ffc-e625e2cd51c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+-----------------------------------------------------------------+------+\n",
      "|sentence                                                |words                                                            |tokens|\n",
      "+--------------------------------------------------------+-----------------------------------------------------------------+------+\n",
      "|Hello I am happy to be learning Apache Spark            |[hello, i, am, happy, to, be, learning, apache, spark]           |9     |\n",
      "|I enjoy learning about Python and javascript Programming|[i, enjoy, learning, about, python, and, javascript, programming]|8     |\n",
      "|I am familiar with machine learning applications        |[i, am, familiar, with, machine, learning, applications]         |7     |\n",
      "|here,is,a,list,of,words                                 |[here, is, a, list, of, words]                                   |6     |\n",
      "+--------------------------------------------------------+-----------------------------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df_token.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee6ceb9b-1563-4627-b6d1-fc9cd463650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = NGram(n=2, inputCol=\"words\", outputCol=\"bigrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4d1dceb5-f319-4cca-8b17-bb40a060b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_df = bigrams.transform(sent_df_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69f7a699-95c5-4dc2-a969-98348492f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|bigrams                                                                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "|[hello i, i am, am happy, happy to, to be, be learning, learning apache, apache spark]                     |\n",
      "|[i enjoy, enjoy learning, learning about, about python, python and, and javascript, javascript programming]|\n",
      "|[i am, am familiar, familiar with, with machine, machine learning, learning applications]                  |\n",
      "|[here is, is a, a list, list of, of words]                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_df.select(\"bigrams\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7e53c2-e19c-40cb-9509-08daca0755f0",
   "metadata": {},
   "source": [
    "### Term Freq-inverse document frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1da6d4f-b956-407e-8e6b-a3e847d0dee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef54697b-a453-424f-bf26-38d732936c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = spark.createDataFrame(\n",
    "    [\n",
    "        (0, 0.0, \"Hello I am happy to be learning Apache Spark\"),\n",
    "        (1, 0.0, \"I enjoy learning about Python and javascript Programming\"),\n",
    "        (2, 1.0, \"I am familiar with machine learning applications\"),\n",
    "        (3, 1.0, \"here,is,a,list,of,words\")\n",
    "    ],\n",
    "    [ 'id', 'label', 'sentence']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ede8e5c6-7e86-4532-a77a-358d6dfb1a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------------------------------------------+\n",
      "|id |label|sentence                                                |\n",
      "+---+-----+--------------------------------------------------------+\n",
      "|0  |0.0  |Hello I am happy to be learning Apache Spark            |\n",
      "|1  |0.0  |I enjoy learning about Python and javascript Programming|\n",
      "|2  |1.0  |I am familiar with machine learning applications        |\n",
      "|3  |1.0  |here,is,a,list,of,words                                 |\n",
      "+---+-----+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sent_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "55d1f49a-ab20-45ed-bc9b-1b228b18c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "words_df = tokenizer.transform(sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2c6c542-4109-447a-ad0c-0e13961a8faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------------------------------------------+-----------------------------------------------------------------+\n",
      "|id |label|sentence                                                |words                                                            |\n",
      "+---+-----+--------------------------------------------------------+-----------------------------------------------------------------+\n",
      "|0  |0.0  |Hello I am happy to be learning Apache Spark            |[hello, i, am, happy, to, be, learning, apache, spark]           |\n",
      "|1  |0.0  |I enjoy learning about Python and javascript Programming|[i, enjoy, learning, about, python, and, javascript, programming]|\n",
      "|2  |1.0  |I am familiar with machine learning applications        |[i, am, familiar, with, machine, learning, applications]         |\n",
      "|3  |1.0  |here,is,a,list,of,words                                 |[here, is, a, list, of, words]                                   |\n",
      "+---+-----+--------------------------------------------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c4a58a15-247c-4896-ae3b-c085a4038cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "featurized = hashingTF.transform(words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9052a982-fcf6-4557-bb7d-1aadc94b6458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+\n",
      "|id |label|sentence                                                |words                                                            |rawFeatures                                                      |\n",
      "+---+-----+--------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+\n",
      "|0  |0.0  |Hello I am happy to be learning Apache Spark            |[hello, i, am, happy, to, be, learning, apache, spark]           |(20,[3,5,6,7,8,9,12,15,16],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|1  |0.0  |I enjoy learning about Python and javascript Programming|[i, enjoy, learning, about, python, and, javascript, programming]|(20,[1,5,9,11,12,14,16],[1.0,1.0,1.0,1.0,1.0,1.0,2.0])           |\n",
      "|2  |1.0  |I am familiar with machine learning applications        |[i, am, familiar, with, machine, learning, applications]         |(20,[0,2,5,10,12,16],[1.0,2.0,1.0,1.0,1.0,1.0])                  |\n",
      "|3  |1.0  |here,is,a,list,of,words                                 |[here, is, a, list, of, words]                                   |(20,[7,8,9,12,15],[1.0,1.0,1.0,1.0,2.0])                         |\n",
      "+---+-----+--------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "featurized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee380c93-08b8-4d96-9ec6-701667c4cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dc61b7ac-56a7-4e65-bd7e-5d9ffb7b9e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(20,[3,5,6,7,8,9,...|\n",
      "|  0.0|(20,[1,5,9,11,12,...|\n",
      "|  1.0|(20,[0,2,5,10,12,...|\n",
      "|  1.0|(20,[7,8,9,12,15]...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescale = idf_model.transform(featurized)\n",
    "rescale.select(\"label\", \"features\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b91107-e54b-49f2-94ff-5fc9770516a0",
   "metadata": {},
   "source": [
    "### Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87340e53-0432-436e-83f3-5da32244730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c4afdad-4d93-4ba8-a2f9-72f88325ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (0, list(\"abcde\")),\n",
    "    (1, list(\"abbbcccdde\"))\n",
    "], [\"id\", \"words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab437177-9442-4fa7-bde2-c25a35c454dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  0|     [a, b, c, d, e]|\n",
      "|  1|[a, b, b, b, c, c...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "225502f0-138d-4f78-928a-11b68814ed2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=5, minDF = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "59d4bfb1-1b4b-4d5b-8288-620688a83a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "32943713-94f7-4575-992f-3ca26531539f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------------------+-------------------------------------+\n",
      "|id |words                         |features                             |\n",
      "+---+------------------------------+-------------------------------------+\n",
      "|0  |[a, b, c, d, e]               |(5,[0,1,2,3,4],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, b, c, c, c, d, d, e]|(5,[0,1,2,3,4],[3.0,3.0,2.0,1.0,1.0])|\n",
      "+---+------------------------------+-------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = model.transform(df)\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe52cae-15ad-4b2c-aa65-3d2ab7932678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
