


from pyspark.sql import SparkSession, functions
import warnings
warnings.filterwarnings('ignore')


spark = SparkSession.builder.appName("stock_price_analysis").getOrCreate()


df = spark.read.csv("hack_data.csv", header=True, inferSchema=True)


df.show(2)


df.describe().toPandas()


from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import VectorAssembler


df.columns


feat_cols = ['Session_Connection_Time',
 'Bytes Transferred',
 'Kali_Trace_Used',
 'Servers_Corrupted',
 'Pages_Corrupted',
 'WPM_Typing_Speed']


assembler = VectorAssembler(inputCols=feat_cols, outputCol='features')


final_df = assembler.transform(df)


from pyspark.ml.feature import StandardScaler


scaler = StandardScaler(inputCol='features', 
                        outputCol='scaled_feat',
                        withStd=True,
                       withMean=False)


scaled_model = scaler.fit(final_df)


cluster_df = scaled_model.transform(final_df)


from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator


eval = ClusteringEvaluator(predictionCol="prediction",
                          featuresCol="scaled_feat",
                          metricName="silhouette",
                          distanceMeasure="squaredEuclidean")


silhouette_score = []
print("""
Silhouette scores for K Means Clustering
========================================
Model\tScore\t
=====\t=====\t
""")
for k in range(2,11):
    kmeans_algo = KMeans(featuresCol='scaled_feat', k=k)
    kmeans_fit = kmeans_algo.fit(cluster_df)
    output = kmeans_fit.transform(cluster_df)
    score = eval.evaluate(output)
    silhouette_score.append(score)
    print(f"K{k}\t{round(score,2)}\t")


import matplotlib.pyplot as plt
fig, ax = plt.subplots(1, 1, figsize=(5,5))
ax.plot(range(2,11), silhouette_score)
ax.set_xlabel("K")
ax.set_ylabel("Score");















